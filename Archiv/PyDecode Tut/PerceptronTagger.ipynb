{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.preprocessing\n",
      "import sklearn.metrics\n",
      "import pydecode\n",
      "import pydecode.model \n",
      "import numpy as np\n",
      "from collections import Counter, defaultdict\n",
      "import itertools\n",
      "from pystruct.learners import StructuredPerceptron"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.max(np.max(np.array([[1,2,3],[32,4]]))) + 1"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 231,
       "text": [
        "33"
       ]
      }
     ],
     "prompt_number": 231
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def viterbi(n, K):\n",
      "    t = max([i for k in K for i in k]) + 1\n",
      "    items = np.arange(n * t, dtype=np.int64)\\\n",
      "        .reshape([n, t])\n",
      "    out = np.arange(n * t * t, dtype=np.int64)\\\n",
      "        .reshape([n, t, t])\n",
      "    c = pydecode.ChartBuilder(items, out)\n",
      "    c.init(items[0, K[0]])\n",
      "    for i in range(1, n):\n",
      "        for t in K[i]:\n",
      "            c.set(items[i, t],\n",
      "                  items[i-1, K[i-1]],\n",
      "                  out=out[i, t, K[i-1]])\n",
      "    return c.finish()"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NGramCoder:\n",
      "    def __init__(self, n_size):\n",
      "        self.tag_encoder = sklearn.preprocessing.LabelEncoder()\n",
      "        self.n_size = n_size\n",
      "\n",
      "    def fit(self, tags):\n",
      "        self.tag_encoder.fit(tags) \n",
      "        return self\n",
      "\n",
      "    def inverse_transform(self, outputs):\n",
      "        y = [None]\n",
      "        for output in sorted(outputs, key=lambda a:a[0]):\n",
      "            y.append(output[1])\n",
      "            if len(y) == 2:\n",
      "                y[0] = output[2]\n",
      "        return self.tag_encoder.inverse_transform(y)\n",
      "\n",
      "    def transform(self, y):\n",
      "        tags = self.tag_encoder.transform(y)\n",
      "        return [(i,) + tuple([tags[i - k] if i - k >= 0 else 0 \n",
      "                              for k in range(self.n_size)])\n",
      "                for i in range(len(y))]"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 233
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coder = NGramCoder(2).fit([\"START\", \"N\", \"D\", \"V\", \"END\"])\n",
      "print coder.transform([\"N\", \"D\", \"N\"])\n",
      "print coder.transform([\"D\", \"D\", \"D\"])"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 2, 0), (1, 0, 2), (2, 2, 0)]\n",
        "[(0, 0, 0), (1, 0, 0), (2, 0, 0)]\n"
       ]
      }
     ],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BigramTagger(pydecode.model.DynamicProgrammingModel):\n",
      "    def __init__(self, tags):\n",
      "        coder = NGramCoder(2)\n",
      "\n",
      "        super(BigramTagger, self).__init__(output_coder=coder)\n",
      "        self.tags = tags + [\"START\", \"END\"]\n",
      "        coder.fit(self.tags)\n",
      "        \n",
      "        self._START = coder.tag_encoder.transform([\"START\"])[0]\n",
      "        self._END = coder.tag_encoder.transform([\"END\"])[0]\n",
      "        self._trans_tags = list(coder.tag_encoder.transform(tags))\n",
      "\n",
      "    def feature_templates(self):\n",
      "        return [(len(self.tags)),\n",
      "                (len(self.tags), len(self.tags))]\n",
      "\n",
      "    def generate_features(self, element, x): \n",
      "        i, tag, prev_tag = element\n",
      "        return [(tag), \n",
      "                (tag, prev_tag)]\n",
      "      \n",
      "    def chart(self, x):\n",
      "        n = len(x)\n",
      "        K = [[self._START]] + [self._trans_tags] * (n - 2) + [[self._END]]\n",
      "        return viterbi(len(x), K)\n",
      "\n",
      "    def loss(self, yhat, y):\n",
      "        print yhat, y\n",
      "        return sklearn.metrics.hamming_loss(yhat, y)\n",
      "\n",
      "    def max_loss(self, y):\n",
      "        return len(y)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 235
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tag_sequences = [tags.split() for tags in [\"D N V\", \"I D N\", \"I D N\"]]\n",
      "data_X = [[\"START\"] + sentence.split()+[\"END\"]  for sentence in \n",
      "          [\"the dog walked\",\n",
      "           \"in the park\",\n",
      "           \"in the dog\"]]\n",
      "data_Y = [[\"START\"] + tags.split() + [\"END\"] for tags in [\"D N V\", \"I D N\", \"I D N\"]]\n",
      "data_X, data_Y"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 237,
       "text": [
        "([['START', 'the', 'dog', 'walked', 'END'],\n",
        "  ['START', 'in', 'the', 'park', 'END'],\n",
        "  ['START', 'in', 'the', 'dog', 'END']],\n",
        " [['START', 'D', 'N', 'V', 'END'],\n",
        "  ['START', 'I', 'D', 'N', 'END'],\n",
        "  ['START', 'I', 'D', 'N', 'END']])"
       ]
      }
     ],
     "prompt_number": 237
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagger = BigramTagger([\"N\", \"V\", \"D\", \"I\"])\n",
      "sp = StructuredPerceptron(tagger, verbose=1, max_iter=3)\n",
      "\n",
      "sp.fit(data_X, data_Y)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration 0\n",
        "['START', 'D', 'N', 'V', 'END'] ['START' 'N' 'N' 'N' 'END']\n",
        "['START', 'I', 'D', 'N', 'END'] ['START' 'D' 'V' 'V' 'END']\n",
        "['START', 'I', 'D', 'N', 'END'] ['START' 'I' 'D' 'N' 'END']\n",
        "avg loss: 0.066667 w: [ 1.  0.  1. -1.  0. -1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  1.  0.  2.  0.  0. -2. -1.  0.  0.  0.  0.  0.  0.  0.\n",
        " -1.  0.  0.  1.  0. -1.]\n",
        "effective learning rate: 1.000000\n",
        "iteration 1\n",
        "['START', 'D', 'N', 'V', 'END']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ['START' 'I' 'D' 'N' 'END']\n",
        "['START', 'I', 'D', 'N', 'END'] ['START' 'D' 'N' 'V' 'END']\n",
        "['START', 'I', 'D', 'N', 'END'] ['START' 'I' 'D' 'N' 'END']\n",
        "avg loss: 0.080000 w: [ 1.  0.  1. -1.  0. -1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  1.  0.  2.  0.  0. -2. -1.  0.  0.  0.  0.  0.  0.  0.\n",
        " -1.  0.  0.  1.  0. -1.]\n",
        "effective learning rate: 1.000000\n",
        "iteration 2\n",
        "['START', 'D', 'N', 'V', 'END'] ['START' 'I' 'D' 'N' 'END']\n",
        "['START', 'I', 'D', 'N', 'END']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ['START' 'D' 'N' 'V' 'END']\n",
        "['START', 'I', 'D', 'N', 'END'] ['START' 'I' 'D' 'N' 'END']\n",
        "avg loss: 0.080000 w: [ 1.  0.  1. -1.  0. -1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  1.  0.  2.  0.  0. -2. -1.  0.  0.  0.  0.  0.  0.  0.\n",
        " -1.  0.  0.  1.  0. -1.]\n",
        "effective learning rate: 1.000000\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 236,
       "text": [
        "StructuredPerceptron(average=False, batch=False, decay_exponent=0,\n",
        "           decay_t0=10, logger=None, max_iter=3,\n",
        "           model=BigramTagger, size_joint_feature: 42, n_jobs=1, verbose=1)"
       ]
      }
     ],
     "prompt_number": 236
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TaggingPreprocessor(pydecode.model.SequencePreprocessor):\n",
      "    WORD = 0\n",
      "    PREFIX_1 = 1\n",
      "    PREFIX_2 = 2\n",
      "    PREFIX_3 = 3\n",
      "    SUFFIX_1 = 4\n",
      "    SUFFIX_2 = 5\n",
      "    SUFFIX_3 = 6\n",
      "\n",
      "    def preprocess_item(self, word):\n",
      "        return [word, word[:1], word[:2], word[:3], word[-3:], word[-2:], word[-1:]]"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 238
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preprocess = TaggingPreprocessor()\n",
      "preprocess.initialize([\"the dog brown\".split(), \"the brown dog\".split()])\n",
      "preprocess.preprocess(\"the dog brown\".split())"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 239,
       "text": [
        "[array([2, 1, 0]),\n",
        " array([2, 1, 0]),\n",
        " array([2, 1, 0]),\n",
        " array([2, 1, 0]),\n",
        " array([2, 0, 1]),\n",
        " array([0, 1, 2]),\n",
        " array([0, 1, 2])]"
       ]
      }
     ],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BetterBigramTagger(pydecode.model.DynamicProgrammingModel):\n",
      "    ENC = TaggingPreprocessor\n",
      "    def __init__(self, tags, pruner=None):\n",
      "        coder = NGramCoder(2)\n",
      "        super(BetterBigramTagger, self).__init__(TaggingPreprocessor(),\n",
      "                                                 output_coder=coder,\n",
      "                                                 pruner=pruner)\n",
      "\n",
      "        self.tags = tags + [\"START\", \"END\"]\n",
      "        coder.fit(self.tags)\n",
      "        \n",
      "        self._START = coder.tag_encoder.transform([\"START\"])[0]\n",
      "        self._END = coder.tag_encoder.transform([\"END\"])[0]\n",
      "        self._trans_tags = list(coder.tag_encoder.transform(tags))\n",
      "\n",
      "    \n",
      "\n",
      "    def feature_templates(self):\n",
      "        def size(t):\n",
      "            return self._preprocessor.size(t)\n",
      "        return [(len(self.tags), size(self.ENC.WORD)),\n",
      "                (len(self.tags), size(self.ENC.SUFFIX_1)),\n",
      "                (len(self.tags), size(self.ENC.SUFFIX_2)),\n",
      "                (len(self.tags), size(self.ENC.SUFFIX_3)),\n",
      "                (len(self.tags), size(self.ENC.PREFIX_1)),\n",
      "                (len(self.tags), size(self.ENC.PREFIX_2)),\n",
      "                (len(self.tags), size(self.ENC.PREFIX_3)),\n",
      "                (len(self.tags), len(self.tags))\n",
      "                ]\n",
      "\n",
      "    def generate_features(self, element, x): \n",
      "        i, tag, prev_tag = element\n",
      "        return [(tag, x[self.ENC.WORD][i]),\n",
      "                (tag, x[self.ENC.SUFFIX_1][i]),\n",
      "                (tag, x[self.ENC.SUFFIX_2][i]),\n",
      "                (tag, x[self.ENC.SUFFIX_3][i]),\n",
      "                (tag, x[self.ENC.PREFIX_1][i]),\n",
      "                (tag, x[self.ENC.PREFIX_2][i]),\n",
      "                (tag, x[self.ENC.PREFIX_3][i]),\n",
      "                (tag, prev_tag),\n",
      "                ]\n",
      "\n",
      "    def chart(self, x):\n",
      "        n = len(x)\n",
      "        K = [[self._START]] + [self._trans_tags] * (n - 2) + [[self._END]]\n",
      "        return viterbi(len(x), K)\n",
      "\n",
      "    def loss(self, yhat, y):\n",
      "        print yhat, y\n",
      "        return sklearn.metrics.hamming_loss(yhat, y)\n",
      "\n",
      "    def max_loss(self, y):\n",
      "        return len(y)\n"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 240
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagger = BetterBigramTagger([\"N\", \"V\", \"D\", \"I\"])\n",
      "sp = StructuredPerceptron(tagger, verbose=1, max_iter=5)\n",
      "sp.fit(data_X, data_Y)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration 0\n",
        "['START', 'D', 'N', 'V', 'END'] ['START' 'N' 'N' 'N' 'END']\n",
        "['START', 'I', 'D', 'N', 'END'] ['START' 'D' 'D' 'V' 'END']\n",
        "['START', 'I', 'D', 'N', 'END']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ['START' 'I' 'D' 'N' 'END']\n",
        "avg loss: 0.053333 w: [ 0.  0.  0. -1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
        "  0.  0.  0.  0.  0.  0.  0.  1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0. -1.  0.  1.  0.  0.  0.  0. -1.  0.  1.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0. -1. -1.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  1.  0.  0.  0.  0.  1. -1.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. -1.\n",
        " -1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. -1.\n",
        "  0.  0.  0.  1.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  1.  0.  0. -1. -1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  1.  0.  0. -1.  0.  0.  0.  0. -1.  0.  1.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1. -1. -1.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  1.  0.  0.  0. -1.  0.  1.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
        "  0.  1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  1.\n",
        "  0.  0.  0. -1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
        "  0.  0.  0.  0.  0.  0.  0.  1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0. -1.  0.  1. -1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  1.  0.  2.  0.  0. -2. -1.  0.  0.  0.  0.  0.  0.  0.\n",
        " -1.  0.  0.  1.  0.  0.]\n",
        "effective learning rate: 1.000000\n",
        "iteration 1\n",
        "['START', 'D', 'N', 'V', 'END'] ['START' 'D' 'N' 'V' 'END']\n",
        "['START', 'I', 'D', 'N', 'END']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ['START' 'I' 'D' 'N' 'END']\n",
        "['START', 'I', 'D', 'N', 'END'] ['START' 'I' 'D' 'N' 'END']\n",
        "avg loss: 0.000000 w: [ 0.  0.  0. -1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
        "  0.  0.  0.  0.  0.  0.  0.  1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0. -1.  0.  1.  0.  0.  0.  0. -1.  0.  1.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0. -1. -1.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  1.  0.  0.  0.  0.  1. -1.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. -1.\n",
        " -1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. -1.\n",
        "  0.  0.  0.  1.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  1.  0.  0. -1. -1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  1.  0.  0. -1.  0.  0.  0.  0. -1.  0.  1.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1. -1. -1.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  1.  0.  0.  0. -1.  0.  1.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
        "  0.  1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  1.\n",
        "  0.  0.  0. -1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
        "  0.  0.  0.  0.  0.  0.  0.  1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0. -1.  0.  1. -1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  1.  0.  2.  0.  0. -2. -1.  0.  0.  0.  0.  0.  0.  0.\n",
        " -1.  0.  0.  1.  0.  0.]\n",
        "effective learning rate: 1.000000\n",
        "Loss zero. Stopping.\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 241,
       "text": [
        "StructuredPerceptron(average=False, batch=False, decay_exponent=0,\n",
        "           decay_t0=10, logger=None, max_iter=5,\n",
        "           model=BetterBigramTagger, size_joint_feature: 330, n_jobs=1,\n",
        "           verbose=1)"
       ]
      }
     ],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sentences(file):\n",
      "    sentence = []\n",
      "    for l in open(file):\n",
      "        t = l.strip().split()\n",
      "        if len(t) == 2:\n",
      "            sentence.append(t)\n",
      "        else:\n",
      "            yield sentence\n",
      "            sentence = []\n",
      "    yield sentence"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sents = [zip(*sentence) for sentence in sentences(\"data/tag_train_small.dat\")]  \n",
      "# X, Y = zip(*sents)\n",
      "# tags = set()\n",
      "# for t in Y:\n",
      "#     tags.update(t)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "outputs": [],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tagger = BetterBigramTagger(list(tags))\n",
      "# sp = StructuredPerceptron(tagger, verbose=1, max_iter=5)\n",
      "# with warnings.catch_warnings():\n",
      "#     warnings.simplefilter(\"ignore\")\n",
      "#     sp.fit(X, Y)\n"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DictionaryPruner:\n",
      "    def __init__(self, limit=1):\n",
      "        self._limit = limit\n",
      "    \n",
      "    def initialize(self, X, Y, output_coder):\n",
      "        self._word_tag_counts = defaultdict(Counter)\n",
      "        self._word_counts = Counter()\n",
      "        for x, y in itertools.izip(X, Y):\n",
      "            elements = output_coder.transform(y)\n",
      "            for element in elements:\n",
      "                self._word_tag_counts[x[element[0]]][element[1]] += 1\n",
      "                self._word_counts[x[element[0]]] += 1\n",
      "        self._all_tags = range(output_coder.n_size)\n",
      "\n",
      "    def table(self, x):\n",
      "        table = [[0]]\n",
      "        for word in x:\n",
      "            if self._word_counts[word] < self._limit:\n",
      "                table.append(self._all_tags)\n",
      "            else:\n",
      "                table.append(self._word_tag_counts[word].keys())\n",
      "        return table"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pruner = DictionaryPruner()\n",
      "coder = NGramCoder(2).fit([\"N\", \"V\"])\n",
      "pruner.initialize([[\"hi\", \"you\"]], [[\"N\", \"V\"]], coder)\n",
      "pruner.table([\"hi\", \"you\", \"hi\"])"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 246,
       "text": [
        "[[0], [0], [1], [0]]"
       ]
      }
     ],
     "prompt_number": 246
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class PrunedBigramTagger(BetterBigramTagger):\n",
      "    def chart(self, x):\n",
      "        table = self._pruner.table(x)\n",
      "        return viterbi(len(x), table)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 247
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def pruned_bigram_tagger(n, tag_sets):\n",
      "#     max_size = max([max(tag_set) for tag_set in tag_sets])\n",
      "#     c = pydecode.ChartBuilder(item_set=pydecode.IndexSet((n+2, max_size + 1)), \n",
      "#                         output_set=output_set(n, max_size + 1))\n",
      "#     for tag in tag_sets[0]:\n",
      "#         c[0, tag] = c.init()\n",
      "\n",
      "#     for i in range(1, n+1):\n",
      "#         for tag in tag_sets[i]:\n",
      "#             c[i, tag] = \\\n",
      "#                 [c.merge((i-1, prev), values=[(i-1, tag, prev)])\n",
      "#                  for prev in tag_sets[i-1]]\n",
      "\n",
      "#     c[n+1, 0] = [c.merge((n, prev), values=[]) \n",
      "#                  for prev in tag_sets[n]]\n",
      "#     return c"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tagger = PrunedBigramTagger(list(tags), pruner = DictionaryPruner(1000))\n",
      "# sp = StructuredPerceptron(tagger, verbose=1, max_iter=3)\n",
      "# with warnings.catch_warnings():\n",
      "#     warnings.simplefilter(\"ignore\")\n",
      "#     sp.fit(X[:50], Y[:50])"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output = sp.predict(X[:100])\n",
      "# output\n",
      "# import sklearn.metrics\n",
      "# sklearn.metrics.hamming_loss([o.tolist() for o in output][:100], Y[:100])"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 250
    }
   ]
  }
 ]
}